---
layout: post
title: Nginx学习初步
category: Nginx
tags: [Nginx,Apache,epoll]
---

### 简介

Nginx（发音同engine x）是一款由俄罗斯程序员Igor Sysoev所开发轻量级的网页服务器、反向代理服务器以及电子邮件（IMAP/POP3）代理服务器。

Nginx相较于Apache、lighttpd具有占有内存少，稳定性高等优势，并且依靠并发能力强，丰富的模块库以及友好灵活的配置而闻名。 在Linux操作系统下，nginx使用epoll事件模型，得益于此，nginx在Linux操作系统下效率相当高。

建议使用NAMP架构（Nginx+Apache+Mysql+PHP），即使用Nginx处理静态请求，并将动态请求反向代理给Apache处理。

> 静态请求Nginx + 动态请求Apache。我理解这种架构就相当于是nginx来做负载均衡，然后直接处理静态请求，并把动态请求转发给apache，两种集群是可以共存的，nginx在前apache在后。

Nginx并不为每一个网页请求创建新的进程或线程。即使负载加大后，内存方面的使用仍是可以预测的。

Nginx还可以作为反向代理系统来使用。该反向代理系统用来对几个后端服务器之间的Web请求进行负载均衡，或者为一台速度比较慢的后端服务器提供缓存机制。

Nginx选择了 epoll and kqueue作为开发模型。

Nginx代码完全用C语言从头写成，已经移植到许多体系结构和操作系统，包括：Linux、FreeBSD、Solaris、Mac OS X、AIX以及Microsoft Windows。

### 与Apache对比

#### nginx 相对 apache 的优点

轻量级，同样起web 服务，比apache 占用更少的内存及资源

抗并发，nginx 处理请求是异步非阻塞的，而apache 则是阻塞型的，在高并发下nginx 能保持低资源低消耗高性能

高度模块化的设计，编写模块相对简单

社区活跃，各种高性能模块出品迅速啊

Nginx 静态处理性能比 Apache 高 3倍以上

反向代理，现在大型网站分工详细，哪些服务器处理数据流，哪些处理静态文件，这些谁指挥，一般都是用nginx反向代理到内网服务器，这样就起到了负载均衡分流的作用。

#### apache 相对nginx 的优点：

rewrite ，比nginx 的rewrite 强大

模块超多，基本想到的都可以找到

少bug ，nginx 的bug 相对较多

超稳定

#### 对比总结

一般来说，需要性能的web 服务，用nginx 。如果不需要性能只求稳定，那就apache 吧。后者的各种功能模块实现得比前者，例如ssl 的模块就比前者好，可配置项多。
这里要注意一点，epoll(freebsd 上是 kqueue )网络IO 模型是nginx 处理性能高的根本理由，但并不是所有的情况下都是epoll 大获全胜的，如果本身提供静态服务的就只有寥寥几个文件，apache 的select 模型或许比epoll 更高性能。当然，这只是根据网络IO 模型的原理作的一个假设，真正的应用还是需要实测了再说的。

apache是阻塞多进程模型，一个连接对应一个进程；nginx是非阻塞的，多个连接（万级别）可以对应一个进程。

现在看有好多集群站，前端nginx抗并发，后端apache集群，配合的也不错。（我理解这种架构就相当于是nginx来做负载均衡，然后直接处理静态请求，并把动态请求转发给apache）

### epoll原理

#### 什么是流？

首先我们来定义流的概念，一个流可以是文件，socket，pipe等等可以进行I/O操作的内核对象。
不管是文件，还是套接字，还是管道，我们都可以把他们看作流。

#### 阻塞 & 非阻塞

之后我们来讨论I/O的操作，通过read，我们可以从流中读入数据；通过write，我们可以往流写入数据。现在假定一个情形，我们需要从流中读数据，但是流中还没有数据，（典型的例子为，客户端要从socket读如数据，但是服务器还没有把数据传回来），这时候该怎么办？

1. 阻塞。阻塞是个什么概念呢？比如某个时候你在等快递，但是你不知道快递什么时候过来，而且你没有别的事可以干（或者说接下来的事要等快递来了才能做）；那么你可以去睡觉了，因为你知道快递把货送来时一定会给你打个电话（假定一定能叫醒你）。

2. 非阻塞忙轮询。接着上面等快递的例子，如果用忙轮询的方法，那么你需要知道快递员的手机号，然后每分钟给他挂个电话：“你到了没？”

当进行非阻塞I/O调用时，要读到完整的数据，应用程序需要进行多次轮询，才能确保读取数据完成，以进行下一步的操作。

很明显一般人不会用第二种做法，不仅显很无脑，浪费话费不说，还占用了快递员大量的时间。
大部分程序也不会用第二种做法，因为第一种方法经济而简单，经济是指消耗很少的CPU时间，如果线程睡眠了，就掉出了系统的调度队列，暂时不会去瓜分CPU宝贵的时间片了。

#### 缓冲区状态事件

为了了解阻塞是如何进行的，我们来讨论缓冲区，以及内核缓冲区，最终把I/O事件解释清楚。缓冲区的引入是为了减少频繁I/O操作而引起频繁的系统调用（你知道它很慢的），当你操作一个流时，更多的是以缓冲区为单位进行操作，这是相对于用户空间而言。对于内核来说，也需要缓冲区。
假设有一个管道，进程A为管道的写入方，Ｂ为管道的读出方。

1. 假设一开始内核缓冲区是空的，B作为读出方，被阻塞着。然后首先A往管道写入，这时候内核缓冲区由空的状态变到非空状态，内核就会产生一个事件告诉Ｂ该醒来了，这个事件姑且称之为“缓冲区非空”。

2. 但是“缓冲区非空”事件通知B后，B却还没有读出数据；且内核许诺了不能把写入管道中的数据丢掉这个时候，Ａ写入的数据会滞留在内核缓冲区中，如果内核也缓冲区满了，B仍未开始读数据，最终内核缓冲区会被填满，这个时候会产生一个I/O事件，告诉进程A，你该等等（阻塞）了，我们把这个事件定义为“缓冲区满”。

3. 假设后来Ｂ终于开始读数据了，于是内核的缓冲区空了出来，这时候内核会告诉A，内核缓冲区有空位了，你可以从长眠中醒来了，继续写数据了，我们把这个事件叫做“缓冲区非满”

4. 也许事件已经通知了A，但是A也没有数据写入了，而Ｂ继续读出数据，知道内核缓冲区空了。这个时候内核就告诉B，你需要阻塞了！，我们把这个时间定为“缓冲区空”。

这四个情形涵盖了四个I/O事件，缓冲区满，缓冲区空，缓冲区非空，缓冲区非满（注都是说的内核缓冲区，且这四个术语都是我生造的，仅为解释其原理而造）。这四个I/O事件是进行阻塞同步的根本。（如果不能理解“同步”是什么概念，请学习操作系统的锁，信号量，条件变量等任务同步方面的相关知识）。

#### 阻塞的缺点

然后我们来说说阻塞I/O的缺点。但是阻塞I/O模式下，一个线程只能处理一个流的I/O事件。如果想要同时处理多个流，要么多进程(fork)，要么多线程(pthread_create)，很不幸这两种方法效率都不高。

#### 非阻塞轮询

于是再来考虑非阻塞轮询的I/O方式，我们发现我们可以同时处理多个流了（把一个流从阻塞模式切换到非阻塞模式再此不予讨论）：

    while true {
        for i in stream[]; {
            if i has data
            read until unavailable
        }
    }

我们只要不停的把所有流从头到尾问一遍，又从头开始。这样就可以处理多个流了，但这样的做法显然不好，因为如果所有的流都没有数据，那么只会白白浪费CPU。这里要补充一点，阻塞模式下，内核对于I/O事件的处理是阻塞或者唤醒，而非阻塞模式下则把I/O事件交给其他对象（后文介绍的select以及epoll）处理甚至直接忽略。

#### select代理

为了避免CPU空转，可以引进了一个代理（一开始有一位叫做select的代理，后来又有一位叫做poll的代理，不过两者的本质是一样的）。这个代理比较厉害，可以同时观察许多流的I/O事件，在空闲的时候，会把当前线程阻塞掉，当有一个或多个流有I/O事件时，就从阻塞态中醒来，于是我们的程序就会轮询一遍所有的流。代码长这样:

    while true {
        select(streams[])
            for i in streams[] {
                if i has data
                read until unavailable
            }
    }

于是，如果没有I/O事件产生，我们的程序就会阻塞在select处。但是依然有个问题，我们从select那里仅仅知道了，有I/O事件发生了，但却并不知道是那几个流（可能有一个，多个，甚至全部），我们只能无差别轮询所有流，找出能读出数据，或者写入数据的流，对他们进行操作。
但是使用select，我们有O(n)的无差别轮询复杂度，同时处理的流越多，每一次无差别轮询时间就越长。

#### epoll是比select更强大的代理，可以将哪些流的事件通知

说了这么多，终于能好好解释epoll了

epoll可以理解为event poll，不同于忙轮询和无差别轮询，epoll之会把哪个流发生了怎样的I/O事件通知我们。此时我们对这些流的操作都是有意义的。（复杂度降低到了O(k)，k为产生I/O事件的流的个数，也有认为O(1)的）

#### 总结一下

对于流的读写的场景，由于读和写是不可预计的，就可能发生我想读的时候缓冲区中没数据，或者我想写的时候缓冲区已经满了。解决这种协作问题，有两种方式：

1. 阻塞：如果流（也可以说缓冲区）不满足我当前读或写的条件，我就等着。等到你满足条件的时候你就唤醒我，我就继续读或写。这种模式的缺点在于，对于每个流都需要独占一个线程。

2. 非阻塞：还是同样的场景，我发现一些流不满足我读或写的条件，我就轮询这些流中的每个，一直问好了没好了没，这样一个好处是可以用一个线程（就是这个一直问好了没好了没的轮询线程）来管理所有流。但是问题在于这种方式很傻，会造成CPU做无用功，因为轮询是需要消耗CPU的。

不过有种方式能解决这种问题，就是将I/O事件托管给select代理，当其中有个流产生了I/O事件之后，轮询线程再开始工作，循环一圈所有需要轮询的流，看哪个现在满足读或写的条件了。不过这还有改进空间，就是如果能再告知到底是哪个流满足了条件，就可以省去这次循环了，这也就是epoll。

-----
2015.3.28更新

### request限制

nginx可以做request频率限制
<http://www.ttlsa.com/nginx/nginx-limiting-the-number-of-requests-ngx_http_limit_req_module-module/>
如果做负载均衡，搞两台nginx，每台10r/s，那么两台就是20r/s。
另外它支持白名单，那么可以把我们自己的压测机器设置到白名单中，才能做压测。

如果还想做更牛逼的request限制的话，可能需要redis进行kv的计数，然后后端接nginx，然后再后端用nodejs（现在这层用的是tornado服务器，用python开发，不过它的社区现在越来越不活跃了。）

-EOF-
